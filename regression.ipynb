{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "717d24c2-3097-40df-884c-642efbf20478",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059cc42a-2f70-497f-95ef-c43ff007dd7a",
   "metadata": {},
   "source": [
    "### Introduction to Notebook\n",
    "\n",
    "***\n",
    "In this notebook, I will explore the concepts of Linear Regression, its practical applications, and how it can be leveraged to make predictions based on data.\n",
    "\n",
    "Our exploration is divided into three comprehensive sections:\n",
    "\n",
    "**Section 1: Introduction to Linear Regression**\n",
    "In this initial section, I will lay the groundwork by introducing the fundamental concepts of Linear Regression. We will understand how this statistical method forms the basis for predicting a target variable based on one or more predictor variables. By establishing a solid understanding of Linear Regression, I will set the stage for our subsequent investigations.  \n",
    "\n",
    "**Section 2: Predicting Broad Jump Length from Weight in the NFL Combine**\n",
    "In the second section, I delve into a practical case study that involves Linear Regression. I will utilize Python and its powerful libraries, such as Pandas, Matplotlib, Seaborn, and Scipy, to examine a dataset containing weight and broad jump length data of players from the NFL Combine in 2020. By employing Linear Regression techniques, I aim to uncover whether there exists a predictive relationship between a player's weight and their performance in the broad jump. Through code-driven analysis and visualization, I will explore the insights provided by this data.\n",
    "\n",
    "**Section 3: Predicting NFL Touchdowns**\n",
    "In the final section, I expand our horizons further by applying Linear Regression to predict a more complex outcome: NFL touchdowns. Building on the experience from the previous section, I will use Scikit-Learn, a powerful machine learning library, to process, clean, and analyze the data related to NFL touchdowns. By training a Linear Regression model, I will aim to predict the number of touchdowns a player(Quarterbacks specfically) will score based on various factors. This section showcases the broader applicability of Linear Regression to diverse datasets and predictive scenarios.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a1eb1-ae52-4f39-a0ce-f76fa048bcc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 1: Introduction to Linear Regression\n",
    "Simple linear regression serves as a tool to gauge the connection between two quantitative variables. This technique comes into play when you wish to uncover:\n",
    "\n",
    "**Strength of Relationship:** This refers to the degree of association or correlation between two variables in a dataset. It quantifies how closely the values of one variable change in response to changes in another variable. For Example, in the context of NFL data analysis, I can examine the strength of the relationship between a player's weight and their broad jump length.\n",
    "\n",
    "**Dependent Variable Value Estimation:** This involves using a regression model to predict or estimate the value of a dependent variable based on one or more independent variables. In linear regression, the goal is to fit a line (or hyperplane in higher dimensions) that best represents the relationship between the independent and dependent variables. Once the model is established, it can be used to estimate the value of the dependent variable for new or unseen data points. This process is particularly useful when you want to forecast an outcome based on known predictor variables.\n",
    "For Example, in the context of NFL data analysis, estimating what quarterbacks will score the most touchdowns next year and how many will they score.\n",
    "\n",
    "These regression models explain the interplay between variables by fitting a line to observed data. Linear regression opts for a straight line, while other forms like logistic and nonlinear regression employ curved lines. Essentially, regression empowers you to gauge the way a dependent variable changes in response to shifts in independent variable(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa89ed8-3ebf-45d4-bddb-f9ae1bca30ea",
   "metadata": {},
   "source": [
    "##### Assumptions of Simple Linear Regression\n",
    "\n",
    "***\n",
    "\n",
    "The foundation of simple linear regression lies on a few assumptions. These include:\n",
    "\n",
    "**Homogeneity of Variance (Homoscedasticity):** The prediction error's size remains relatively constant across different independent variable values.\n",
    "\n",
    "**Independence of Observations:** The dataset's observations are collected using statistically valid methods, devoid of concealed relationships among them.\n",
    "\n",
    "**Normality:** The data conforms to a normal distribution.\n",
    "\n",
    "**Linearity of Relationship:** The connection between the independent and dependent variables adheres to a linear pattern, manifesting as a straight line among data points rather than a curve or a grouping pattern.\n",
    "Should the data fail to meet the criteria of homoscedasticity or normality, a nonparametric test such as the Spearman rank test could serve as a suitable alternative. Furthermore, if the data contradicts the assumption of independent observations (say, due to repeated observations over time), a linear mixed-effects model might be a better approach, accounting for the extra structure within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f62519-18b6-4fb4-a499-0236e6dd72a4",
   "metadata": {},
   "source": [
    "##### Conducting a Simple Linear Regression Test\n",
    "\n",
    "Here's a snapshot of the process:\n",
    "\n",
    "**The Formula:** The formula for simple linear regression stands as follows:\n",
    "\n",
    "y = β0 + β1X + ε\n",
    "\n",
    " - y represents the predicted dependent variable value (y) for any given independent variable value (x).\n",
    " - β0 signifies the intercept – the forecasted y value when x equals 0.\n",
    " - β1 indicates the regression coefficient, indicating how y changes as x increases.\n",
    " - x stands as the independent variable, exerting its influence on y.\n",
    " - ε denotes the estimation error, revealing the extent of variation in the regression coefficient estimate.\n",
    "Finding the Best-Fit Line: Linear regression seeks the best-fit line by determining the regression coefficient (β1) that minimizes the overall model error (ε).\n",
    "\n",
    "Although manual execution of linear regression is doable, it tends to be an extremly boring and labour intensive task. Most individuals leverage statistical software such as the python packages sckit-learn or scipy to efficiently analyze their data.[[1]](https://www.scribbr.com/statistics/simple-linear-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3627f5-c719-4240-9c51-9a1335fd722f",
   "metadata": {},
   "source": [
    "### Section 2: How Weight can Predict the Broad Jump of players in the NFL combine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2a797-ebbc-4cda-a758-bfb024651902",
   "metadata": {},
   "source": [
    "In this section I am going to use linear regression on a csv file downloaded from kaggle[[1]]('https://www.kaggle.com/datasets/mrframm/nfl-2020-combine') that contains the data of the players that were invited to the 2020 NFL combine. I have tried doing regression models on NFL data before, however I never really got very far as there are so many variables that pridicting anything is a bit of a nightmare and it gets extremly complex very quickly. However for the purposes of this notebook and demonstration of simple regression I am going to clean a dataframe and simply determine if we can predict the Broad jump of a player based on his weight. (Broad jump is just another term for long jump...Americans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26c96b-bb9a-4011-bf22-9d30d7b355c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Statistical arrays\n",
    "import numpy as np\n",
    "\n",
    "#Statistical tools\n",
    "import scipy.stats as ss\n",
    "\n",
    "# Optimization\n",
    "import scipy.optimize as so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416123d1-3c1a-45c0-8abc-11aafa6a02b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "cb_data = pd.read_csv(\"./Data/NFL_Combine_2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf2501-3055-4a34-bad3-939c9a392cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peek\n",
    "cb_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3cf26f-213d-4a04-8d29-e3b35a05b179",
   "metadata": {},
   "source": [
    "First thing I notice in the data is that some of the players have not taken part in each event as there are empty cells from my data (NaNs). Many of these player (especially the top athletes) do not participate in the combine as they do not need to - example above is Chase Young - in 2019/2020 he was a top recruit coming out of college so he did not take part in any events. For the purposes of this excercise I am going the exclude the data of any player that did not take part in the broad jump events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634df81b-bd72-4187-8346-75aebbc750e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original row count\n",
    "print(\"No. of rows before NaNs removed: \", len(cb_data.index))\n",
    "\n",
    "# Drop rows containing NaNs in 'Wt' or 'Broad Jump' columns\n",
    "cb_data = cb_data.dropna(subset=['Wt', 'Broad Jump'])\n",
    "\n",
    "# Get Row Count of players after cleaning\n",
    "print(\"No. of rows after NaNs removed: \", len(cb_data.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb10d67-2d25-4c9e-949e-28146f45a49f",
   "metadata": {},
   "source": [
    "Above I have removed the 98 players from the dataframe that did not have the neccessary data recorded for the investigation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46674e01-f73a-4c04-a887-2123bfccb610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at a sample\n",
    "cb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946821a-8438-444b-987b-59417eccdb49",
   "metadata": {},
   "source": [
    "### Plot the Data\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f92a4-aabc-4a38-8abd-443d1b35d8d1",
   "metadata": {},
   "source": [
    "Here I am going to create a scatter plot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06948a-a22d-41da-9342-ee2ddac7116c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create x and y data from data frame\n",
    "x = cb_data['Wt'].tolist()\n",
    "y = cb_data['Broad Jump'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82bd48-f1ec-44ef-abbc-08c81a541647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "fig, ax = plt.subplots(figsize = (12,6))\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('Weight (lbs)')\n",
    "plt.ylabel(\"Jump Height (cm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd06df6a-0e47-40a3-8ead-55c8eb367488",
   "metadata": {},
   "source": [
    "So here I can see that the data points are indicating a negative relationship between these two variables as the data points are on a downward trajectory from the top left to the bottom right of the plot. Or as the weight of a player increases, the shorter the jump tends to get.\n",
    "\n",
    "Below I will use the linregress tool in scipy to get the pvalue of the data[[2]]('https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html'). This is to prove that the values of a players weight (x) is directly related to the players broadjump.  \n",
    "The linregress function in the scipy.stats module is a convenient tool for performing linear regression analysis on a set of data points, it calculates several important parameters of the linear regression model, including the slope, intercept, correlation coefficient (r-value), p-value, and standard error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2911b2-c1cd-418c-b531-2f4b4b6c9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ss.linregress(x, y)\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabbde8-c86e-4161-afbd-ca93113aa9c1",
   "metadata": {},
   "source": [
    "After performing the linear regression analysis, the linregress function returns a LinregressResult object, which contains several statistical values that describe the linear regression model. Here I will go through some of the results we will use in our model. \n",
    "\n",
    "**pvalue:** This is the p-value associated with the null hypothesis that there is no significant linear relationship between the variables x and y. A small p-value (usually less than 0.05) indicates that the linear relationship is statistically significant. In the output, the pvalue is approximately 3.930794374293921e-35, which is extremely small (almost 0) and suggests a highly significant linear relationship.(This is not actually used in the model but the miniscule score gives creedence to generating a model.)\n",
    "\n",
    "**slope:** This is the slope of the linear regression line, which represents the change in the dependent variable (y) for a unit change in the independent variable (x). In the output, the slope is approximately -0.1385.\n",
    "\n",
    "**intercept:** This is the intercept of the linear regression line, which is the value of the dependent variable (y) when the independent variable (x) is zero. In the output, the intercept is approximately 151.8988.\n",
    "\n",
    "Now using the results above I will create a the preditive data to fit the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc49153-9197-499f-ba24-2094d8887572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating predictive data\n",
    "y_pred = res.intercept + res.slope * np.array(x)\n",
    "\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(figsize = (12,6))\n",
    "\n",
    "# PLot x and y\n",
    "ax.plot(x,y, 'k.')\n",
    "\n",
    "# PLot x and predictive data(y_pred) on x\n",
    "ax.plot(x, y_pred)\n",
    "\n",
    "# x and y axis labels\n",
    "plt.xlabel('Weight (lbs)')\n",
    "plt.ylabel(\"Jump Height (cm)\")\n",
    "\n",
    "ax.plot(x,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbfb752-75fb-4a72-8b44-a8fea0e721ec",
   "metadata": {},
   "source": [
    "### Linear Regression using the polyfit function\n",
    "Below I will do the same thing, except this time instead of creating a formula to fit the line, I will ask they polyfit function in the scipy library to do this for me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4642e1e2-38e7-44e2-9be5-f5d6f519fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the polyfit function the data needs to be in numpy arrays\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "\n",
    "# use x and y to find original parameters used to create data\n",
    "params = np.polyfit(x, y, 1)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6629fa7d-6ef5-4da5-9877-e9a4a0372e57",
   "metadata": {},
   "source": [
    "The first value -1.38524107e-01  is the slope of the linear regression line. This value indicates the rate of change of the dependent variable (y) with respect to a unit change in the independent variable (x). In this case, as expected since we saw this above, the negative value suggests a negative correlation between x and y, meaning that as x increases, y tends to decrease.\n",
    "\n",
    "The second value 1.51898839e+02 is the intercept of the linear regression line. It represents the value of the dependent variable (y) when the independent variable (x) is zero. In this case, it suggests that when x is zero, the predicted value of y is approximately 156.36.\n",
    "\n",
    "The e notation in the values represents scientific notation. For example, 1.38524107e-01 is equivalent to -0.138524107, and 1.51898839e+02 is equivalent to 151.898839.  \n",
    "You will notice these values are almost identical to the values returned to me when I ran the linregress function in the previous section (the slope and intercept outputs).  \n",
    "\n",
    "In summary, the params array provided represents the coefficients of the linear regression line that best fits the data points (x, y). \n",
    "Both functions are attempting to achieve the same goal—fitting a linear regression line to the data and extracting the slope and intercept of that line. The slight numerical differences in the results might be due to different algorithms or methods used by the linregress and polyfit functions to perform the regression, but the essence of what they are doing is the same.  \n",
    "The linear equation that describes this relationship is approximately:  \n",
    "**y=−0.1385x+151.8988**  \n",
    "This equation represents the best-fit line that has been determined through linear regression using my data. It indicates that for each unit increase in x, y is expected to decrease by approximately 0.1385 units, and when x is 0, y is expected to be approximately 151.8988.\n",
    "This equation can be used to make predictions based on the given data and the fitted linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e403a5d-1cfd-416f-9d26-cc67ca1c03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create a plot\n",
    "fig, ax = plt.subplots(figsize = (12,6))\n",
    "\n",
    "# PLot x and y\n",
    "ax.plot(x,y, 'k.')\n",
    "\n",
    "\n",
    "# PLot x and params on x\n",
    "ax.plot(x, params[0] * x + params[1], 'r-')\n",
    "\n",
    "# Labels\n",
    "plt.xlabel('Weight (lbs)')\n",
    "plt.ylabel(\"Jump Height (cm)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a16b4e9-9d9c-4036-84d9-ee9883b8fa27",
   "metadata": {},
   "source": [
    "Now realistically some context has to be put on this data. Theses are professional athletes, in the real world with \"normal\" people who do not train for hours almost every day the data may have a lot more varience. Plus, these are people, there does come a point of diminishing returns where if a person does not weigh enough they will not have the strength or power/energy to jump. In the plot above we may even see that point at roughly 190-200 lbs. If we had a lager sample with lighter athletes, it is very possible that the data would have a more \"normal\" or bell curve distribution rather than linear.  \n",
    "Also in a larger sample that includes a broader range of individuals, including those who are not professional athletes, you might see more variation in the data. Again this variation could lead to a more bell-shaped distribution, reflecting the diverse range of physical abilities and training levels in the general population.  \n",
    "Considering these factors, it's important to remember approach data analysis and modeling with a nuanced understanding of the domain being studying. While linear regression can provide valuable insights, it's not always the best fit for every situation. Exploring alternative models, such as polynomial regression, quadratic regression, or other curve-fitting methods, could help capture more complex relationships that exist in real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df06ae-bda5-4dfc-8a6d-bd48c1b428a1",
   "metadata": {},
   "source": [
    "Below I have created a graph that allows a polynomial of degree 3 (It is now a cubic polynomial. 3 is just a default - users can play with this, just change the \"deg\" value below) to the given data points (x, y). This means that the polynomial will have significantly more flexibility and complexity, and it will try to match the fluctuations and details in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc143c9-4172-47a6-ad19-8f80b20029c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degrees of the polynomial(Freedom)\n",
    "deg = 3\n",
    "\n",
    "# Use x and y to try to find the original parameters\n",
    "params = np.polyfit(x, y, deg)\n",
    "\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(figsize = (12,6))\n",
    "\n",
    "#plot x and y\n",
    "ax.plot(x, y, 'k.')\n",
    "\n",
    "# create temp x values\n",
    "xtmp = np.linspace (170.0, 400.0, 1000)\n",
    "\n",
    "# PLot x and params on x\n",
    "ax.plot(xtmp, np.polyval(params, xtmp), 'b-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a8f57-6842-4b93-a440-1235af99ffa5",
   "metadata": {},
   "source": [
    "When you increase the degree of the polynomial, it can capture intricate patterns and variations in the data, potentially resulting in a curve that passes very closely to each data point. However, there is a risk of overfitting, which means that the polynomial might capture noise in the data rather than the true underlying trend. This can lead to poor generalization when making predictions on new, unseen data.\n",
    "\n",
    "The choice of the polynomial degree should be based on the complexity of the underlying relationship in the data and the goals of the analysis. There is a trade-off between fitting the data well and avoiding overfitting. In many cases, lower-degree polynomials are preferred to avoid overfitting, unless there is strong justification for using a higher degree polynomial.  \n",
    "In the investigation I have done above into the relationship between an athletes weight and length of broad jump we can see how expanding the degrees of the polynomial gives a little bit of a more accurate picture in some ways as we can see the curve start to trend down as it approaches the y-axis, however contrary to that the blue line looks like it is begging to trend back up in as it is going away from the x-axis, and the likelyhood of anyone, even a pro athlete being able to jump highter the closer they get to 375lbs is not realistic. That last data point is a bit of an outlier that may be throwing off the model - anyone that is 350 lbs and can jump over 110 cms is a outlier. An outlier in a stable of outliers.  \n",
    "In my opinion the cubic polynomial (deg = 3) gives the best \"fit\" as an illustration for a presentation, and also for analysis, as long as the presenter/analyset has a clear compreheshion and is able to explain the context and limitations of the data sample. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046ae0d-ea56-488b-9f9d-b8de705ee696",
   "metadata": {},
   "source": [
    "# Section 3: Predict NFL Touchdowns\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9172399-7fa0-4416-8512-552fa31de1ce",
   "metadata": {},
   "source": [
    "Here in this section I am going to attempt to make a model that will predict who will score the most nfl touchdowns next season, and how many will they throw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93e895c-c434-4b18-94ca-3bc8c6e525e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libaries\n",
    "\n",
    "# Pandas for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Module to find pathnames matching a specified pattern\n",
    "import glob\n",
    "\n",
    "# Module for interacting with the operating system\n",
    "import os\n",
    "\n",
    "# NumPy for arrays, matrices, and mathematical functions\n",
    "import numpy as np \n",
    "\n",
    "# Matplotlib for creating various types of plots and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn for statistical data visualization built on Matplotlib\n",
    "import seaborn as sns \n",
    "\n",
    "# IPython widgets for creating interactive user interfaces\n",
    "from ipywidgets import interact, interactive, fixed, Layout, Dropdown, widgets\n",
    "\n",
    "# SciPy for mathematical, scientific, and engineering functions\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58878419-a50b-44f2-af7e-90916046cdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a310c43-549b-46fa-8cf5-292d07ed5516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in multiple csv with same headings to 1 dataframe\n",
    "# https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe\n",
    "path = '.\\Data\\Pred_data'\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbfa992-49f5-44bf-84cd-e00612c200bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96a657e7-1817-47c7-8f36-8ede71d41fe5",
   "metadata": {},
   "source": [
    "### About the Data\n",
    "\n",
    "The data that was imported here was play-by-play data. That means it is a detailed account of every individual play that occurs during a football game. It includes a chronological record of each play for the last 3 seasons, including information about the teams involved, the down and distance, the location on the field, the players on the field, the outcome of the play, and many other details.  \n",
    "However, since a multitude of these records are tangential, at best, to the focus of our investigation, a process of analysis and refinement is required. This entails scrutinizing and \"cleaning\" the data, separating the relevant from the extraneous to ensure our study remains targeted and at least somewhat insightful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996d366-464b-4753-bbb5-9e49b5451b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index as each csv will have its own\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d061c1-c130-41c2-92c6-7ec8f1b7fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peek\n",
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a6207-75d6-473b-8938-3ec5e7ae1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the shape of the dataframe\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8263ac-58f1-4eb5-8e11-155a1d4786db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploring and Grouping the data\n",
    "\n",
    "***\n",
    "\n",
    "As you can see this is a fairly large dataframe, in comparison to what I have been using up to now it is enormous. I will not be using all the data here to create my model so I will now begin cleaning and grouping the data I need. I will look through the column titles and find the colums with the data I want to analyze to see if there is a corrolation between that data and touchdown passess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b4c94d-b020-42e2-ac40-20de27c251fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# list all columns in the dataframe \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# list all columns in the dataframe \n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a099a50-4780-4cd2-9311-603ddf7678fd",
   "metadata": {},
   "source": [
    "### Choosing the Statistics to consider\n",
    "\n",
    "***\n",
    "\n",
    "Below I wil now group the attributes I consider to be releavent to the data I am trying to output from my model. First I will group the players and their stats by the position I am interested in, Quarterback. \n",
    "The following code sets up the statistics to be analyzed for quarterbacks (qb_stats) and specifies the criteria by which the data should be grouped for aggregation (groupby_stats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b26f9-f122-4f0d-8634-a5ef554abd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quarterback statistics to consider\n",
    "qb_stats = [\n",
    "    'season', 'passer_id', 'passer', 'pass', \n",
    "    'complete_pass', 'interception', \n",
    "    'sack', 'yards_gained', 'touchdown', 'qb_dropback', 'wind'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df346a5-b22f-4464-9ca5-9a2fff59056f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grouping criteria\n",
    "groupby_stats = [\n",
    "    'season', 'passer_id', 'passer'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce5aca-9ad4-48b7-83dd-340499fd7904",
   "metadata": {},
   "source": [
    "Now I will group and aggregate quarterback statistics, then select a random sample of 10 player-season combinations for display, providing a glimpse into the aggregated data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03a173-fea5-4cdd-a001-def833335758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Group by and aggregate by sum\n",
    "qb_df = df[qb_stats].groupby(groupby_stats, as_index=False).sum()\n",
    "\n",
    "# Randomly sample 10 player-seasons\n",
    "random_sample = qb_df.sample(n=10)\n",
    "\n",
    "# Display the random sample\n",
    "print(random_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56c41b-a0f7-4105-8dbb-2743487f86eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Aggregating statistics in the context of data analysis refers to the process of summarizing and condensing data values within groups.  \n",
    "The selected subset of columns are grouped by the columns listed in groupby_stats. It groups the data into distinct groups based on unique combinations of values in the specified columns.  \n",
    "\n",
    "First as an example, I will take a single attribute of that I am almost certain has a strong impact on if a touchdown is scored, that is **yards_gained**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe0f3c7-49dc-437e-81c5-9fe71a195e67",
   "metadata": {},
   "source": [
    "### Analyse a single influence\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fabe72-9089-4584-8ac0-aa4649745b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to analyze correlation with touchdowns\n",
    "variables_to_analyze = ['yards_gained']\n",
    "variable = 'yards_gained'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a514176-0b2c-4584-8396-25aae2f83085",
   "metadata": {},
   "source": [
    "As evidence before I begin fitting the data I will genterate the correlation coefficent, or r-value. The correlation coefficient (r-value) measures the strength and direction of a linear relationship between two variables. The value of the correlation coefficient can range from -1 to 1:\n",
    "\n",
    " - If the correlation coefficient is close to 1, it indicates a strong positive linear relationship, meaning that as one variable increases, the other tends to increase as well.  \n",
    " - If the correlation coefficient is close to -1, it indicates a strong negative linear relationship, meaning that as one variable increases, the other tends to decrease.  \n",
    " - If the correlation coefficient is close to 0, it indicates a weak or no linear relationship between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406834c9-0d80-475f-9077-7ecab3712335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate correlation coefficient\n",
    "correlation_coefficient = qb_df['touchdown'].corr(qb_df[variable])\n",
    "\n",
    "#Show result\n",
    "correlation_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d53e547-487c-47d7-ad70-99590e6c51b6",
   "metadata": {},
   "source": [
    "The correlation coefficient of approximately 0.97 indicates a strong positive linear relationship between these two variables. This means that as the number of yards gained increases, the number of touchdowns tends to increase as well. \n",
    "Now I am going to use the linregress function in scipy,stats to calculate the parameters needed to define a linear regression line that best fits the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d827a-b847-4e75-a290-42119788c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression line\n",
    "slope, intercept, r_value, p_value , std_err = stats.linregress(qb_df['touchdown'], qb_df[variable])\n",
    "print('Slope = ', slope)\n",
    "print('Intercept = ', intercept)\n",
    "print('r_value = ', r_value)\n",
    "print('p_value = ', p_value)\n",
    "print('std_err = ',  std_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ce14e-6428-404b-8fe9-92b604b3bb17",
   "metadata": {},
   "source": [
    "From this information I can now create the regression line. This line creates the equation of the linear regression line using the calculated slope and intercept. The equation is y = mx + b, where y is the predicted value (in this case, qb_df['ysrds_gained']), x is the input (qb_df['touchdown']), m is the slope, and b is the intercept.\n",
    "\n",
    "Also it should be noted the p-value is a measure of the evidence against a null hypothesis (the hypothesis that there's no relationship between the variables). A very small p-value (4.3e-218 in this case) suggests strong evidence against the null hypothesis, indicating that the relationship between 'touchdown' and 'yards_gained' is statistically significant. Also standard error value of 1.702 is relatively small, which suggests that the estimated slope of the regression line is reliable. A smaller standard error indicates that the data points are relatively close to the regression line, meaning that the line provides a good fit to the data.\n",
    "\n",
    "The slope of the regression line was calculated to be approximately 125.48. This value represents the estimated change in yards gained for every additional touchdown. It indicates that, on average, an increase of one touchdown is associated with an increase of around 125.48 yards gained.  \n",
    "\n",
    "The intercept of the regression line was determined to be approximately 130.45. This value represents the estimated yards gained when there are no touchdowns. In other words, even without any touchdowns, quarterbacks are estimated to gain around 130.45 yards.  \n",
    "\n",
    "By multiplying the slope with qb_df['touchdown'] and adding the intercept, I get the predicted y-values for each x-value. These predicted values form the points on the regression line, which can then be plotted to visualize how well the line fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff57b6-aa3d-4171-8ef1-4385134ebf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the regressuion line\n",
    "regression_line = slope * qb_df['touchdown'] + intercept\n",
    "\n",
    "#Show predicted y values for each X-value\n",
    "regression_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df309a4-8639-4add-9e7f-43bc7c6957d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the data\n",
    "plt.figure(figsize=(12, 8))  # Set figsize\n",
    "# Create scatter plot\n",
    "plt.scatter(qb_df['touchdown'], qb_df[variable], color='red', label='Data points')\n",
    "\n",
    "#plot the regression line\n",
    "plt.plot(qb_df['touchdown'], regression_line, color='blue', label='Regression line')\n",
    "\n",
    "plt.title(f\"Correlation between Touchdowns and {variable}\\nCorrelation Coefficient: {correlation_coefficient:.2f}\")\n",
    "plt.xlabel('Touchdowns')\n",
    "plt.ylabel(variable)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bce1657-d1a1-434a-9d2f-104d0a7eb850",
   "metadata": {},
   "source": [
    "The scatter plot displays the data points in red, with each point representing a quarterback's touchdowns and corresponding yards gained. The blue regression line is fitted to the data points using the calculated slope and intercept. This line serves as a predictive model for estimating yards gained based on the number of touchdowns.\n",
    "The analysis and visualisation indicate a strong positive correlation between the number of touchdowns and the yards gained by quarterbacks. The regression analysis and visualization provide valuable insights into the relationship between these variables and offer a predictive model for estimating yards gained based on touchdown counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58cec16-353a-43be-a4a8-8e6964f956e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59c29ec5-e106-45a5-af24-5d7c3f4d9846",
   "metadata": {},
   "source": [
    "### Analyse multiable influences\n",
    "\n",
    "***\n",
    "\n",
    "Now I could go doing this, individually, for all the data that I believe has a strong relationship with scoring touchdown passes, however there is a plotting tool  called regplot() in the seaborn libary that will not only plot these graphs with the regression line for me, it will also make them interactive so I can pick the factor I want to analyse by dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332cc2c-3cbd-4f34-aaf2-c1149dc52d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to analyze correlation with touchdowns\n",
    "variables_to_analyze = ['yards_gained', 'complete_pass', 'pass', 'interception', 'sack', 'qb_dropback', 'wind']\n",
    "\n",
    "# Define layout for centering the dropdown\n",
    "centered_layout = Layout(display='flex', justify_content='center')\n",
    "\n",
    "@interact(variable=variables_to_analyze, width=(6, 20), height=(4, 12), layout=centered_layout)\n",
    "def plot_correlation(variable, width, height):\n",
    "    plt.figure(figsize=(width, height))  # Set figsize\n",
    "    \n",
    "    # Set scatter plot properties (dots red)\n",
    "    scatter_kws = {'color': 'red'}\n",
    "    # Set regression line properties (line blue)\n",
    "    line_kws = {'color': 'blue'}\n",
    "    \n",
    "    sns.regplot(data=qb_df, x='touchdown', y=variable, scatter_kws=scatter_kws, line_kws=line_kws)\n",
    "    \n",
    "    # Calculate correlation coefficient\n",
    "    correlation_coefficient = qb_df['touchdown'].corr(qb_df[variable])\n",
    "    \n",
    "    # Calculate p-value\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(qb_df['touchdown'], qb_df[variable])\n",
    "    \n",
    "    plt.title(f\"Correlation between Touchdowns and {variable}\\nr-value: {correlation_coefficient:.2f}, p-value: {p_value:.2e}\")\n",
    "    plt.xlabel('Touchdowns')\n",
    "    plt.ylabel(variable)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d137c75-0d7e-4ce3-9fb9-040c7a8a6d12",
   "metadata": {},
   "source": [
    "In Seaborn, the sns.regplot() function is used to create a scatter plot with a regression line fit through the data points. It allows the user to visually explore the relationship between two variables and estimate the linear relationship between them. The main purpose of regplot is to provide insights into the correlation and trend between two variables, and thankfully it does the linear regression function and fits the line for me.\n",
    "\n",
    "Each dot corresponds to a specific data instance (e.g., a player-season in the dataset).  \n",
    " - The x-coordinate of each dot represents the number of touchdowns scored by the player in that season.  \n",
    " - The y-coordinate of each dot represents the value of the selected variable (e.g., yards gained, completed passes, etc.) for the same player-season.  \n",
    " \n",
    "The blue line is a linear regression line that best fits the scatter plot's data points.  \n",
    "It represents the general trend or relationship between the two variables: touchdowns and the selected variable.\n",
    " - The slope of the line indicates the change in the selected variable's value as the number of touchdowns changes.\n",
    " - If the line has a positive slope, it suggests that higher touchdowns are associated with higher values of the selected variable. Conversely, a negative slope indicates an inverse relationship.\n",
    "\n",
    "The shaded blue section around the regression line represents the range within which the actual regression line is likely to fall with a certain level of confidence.  \n",
    " - It indicates the uncertainty in the estimated regression line due to the variability in the data points.\n",
    " - The wider the shaded blue section, the greater the uncertainty in the regression line's position. Conversely, a narrower shaded section indicates higher confidence in the regression line's position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094df2ce-5c2c-42ae-9064-931d349f628e",
   "metadata": {},
   "source": [
    "### Summary of Analysis\n",
    "\n",
    "**Yards Gained:** As expected, there is a strong positive correlation (r-value: 0.97) between the number of touchdowns and the yards gained by quarterbacks. The p-value is exceedingly small (p-value: 4.3e-218), indicating a highly significant relationship.\n",
    "\n",
    "**Complete Passes:** A strong positive correlation (r-value: 0.95) is observed between touchdowns and the number of completed passes. The relationship is statistically significant (p-value: 9.06e-190).\n",
    "\n",
    "**Pass Attempts:** The number of pass attempts shows a strong positive correlation (r-value: 0.95) with touchdowns, though the relationship is statistically significant (p-value: 3.96e-178).\n",
    "\n",
    "**Interceptions**(QB Throws the ball to the other team): The correlation coefficient is approximately 0.85. This strong positive r-value suggests a significant positive linear relationship between the number of touchdowns and the number of interceptions. As the number of touchdowns increases, the number of interceptions tends to increase as well. The calculated p-value is approximately 5.88e-100. This very small p-value provides compelling evidence against the null hypothesis, confirming the statistical significance of the relationship between touchdowns and interceptions.The calculated slope suggests that an increase in touchdowns corresponds to an increase in interceptions, although the magnitude of this increase is relatively small.\n",
    "\n",
    "**Sacks**(Tackled before throwing the ball): The correlation coefficient is approximately 0.83. This strong positive r-value indicates a notable positive linear relationship between the number of touchdowns and the number of sacks. As the number of touchdowns increases, the number of sacks tends to increase as well. This was surprising to me as I would have thought that the more a player was sacked the less oppertunaties they would have to throw a touch down.  The calculated p-value is approximately 2.10e-91. This extremely small p-value provides compelling evidence against the null hypothesis, confirming the statistical significance of the relationship between touchdowns and sacks.\n",
    "\n",
    "**Quarterback Dropbacks**(The movement a quarterback makes by taking a few steps backward from the position where the ball starts the play): This strong positive r-value(.95) indicates a significant positive linear relationship between the number of touchdowns and the number of quarterback dropbacks. As the number of touchdowns increases, the number of dropbacks tends to increase as well.The calculated p-value is approximately 8.83e-178. This extremely small p-value provides compelling evidence against the null hypothesis, confirming the statistical significance of the relationship between touchdowns and quarterback dropbacks.\n",
    "\n",
    "\n",
    "**Wind Conditions:** This moderate positive r-value (0.62) indicates a positive linear relationship between the number of touchdowns and wind conditions. However, the strength of this relationship is moderate compared to the other variables analyzed.The calculated p-value is approximately 3.53e-40. This small p-value provides compelling evidence against the null hypothesis, confirming the statistical significance of the relationship between touchdowns and wind conditions. Also it is clear from the wind plot that the standard error of the slope is much larger than in the other plots, its value is approximately 3.83. This value indicates the variability of the estimated slope, suggesting some uncertainty in the slope estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc5c4c2-eadf-4ab9-b385-bbf7add41fec",
   "metadata": {},
   "source": [
    "So from this analysis knowing yards gained, completed passes, attempted passes, interceptions, and sacks all correlate with touchdowns. Let's see if this is the case for next seasons touchdown.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae84b7-e312-4cba-ace5-57c6f5b2e3f9",
   "metadata": {},
   "source": [
    "### Analysis for the Next Season:\n",
    "\n",
    "***\n",
    "\n",
    "Now I will analyze the correlations of these statistics with next season's touchdowns by creating a copy of the original quarterback dataframe and increment the 'season' values by 1 to represent the next season.\n",
    "The original and modified dataframes are merged to create a new dataframe ('new_qb_df') that will contain both the statistics for the current season and the corresponding statistics for the next season.\n",
    "The code will again generates similar scatter plots and regression lines for the 'new_qb_df' dataframe to analyze the correlations between the statistics from the current season and the statistics from the next season, specifically in relation to next season's touchdowns.\n",
    "For each statistic, it creates a scatter plot with 'touchdowns' on the x-axis and the corresponding predicted statistic from the next season on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73148bb-9241-443c-9e11-4404164ff8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the qb_df DataFrame\n",
    "_df = qb_df.copy()\n",
    "\n",
    "# Increment the values in the 'season' column of the copied DataFrame by 1\n",
    "_df['season'] = _df['season'].add(1)\n",
    "\n",
    "# Merge the original qb_df with the modified _df based on specified columns\n",
    "new_qb_df = qb_df.merge(_df, on=['season', 'passer_id', 'passer'],\n",
    "                        suffixes=('', '_prev'),\n",
    "                        how='left')\n",
    "# Display a random sample of 10 rows from the merged DataFrame\n",
    "new_qb_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3acdb62-224b-47ac-a66a-582dd6de6a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a97f1-f62a-4d80-bb87-569d8835aa51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf30807-a988-4060-a2fc-f02e3c5d18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_analyze = ['touchdown_prev', 'yards_gained_prev', \n",
    "                        'complete_pass_prev', 'pass_prev', \n",
    "                        'interception_prev', 'sack_prev', 'qb_dropback_prev']\n",
    "\n",
    "centered_layout = widgets.Layout(display='flex', justify_content='center')\n",
    "\n",
    "@interact(y=variables_to_analyze, width=(6, 20), height=(4, 12), layout=centered_layout)\n",
    "def plot_interactive_correlation(y, width, height):\n",
    "    plt.figure(figsize=(width, height))  # Set figsize\n",
    "    \n",
    "    scatter_kws = {'color': 'red'}  # Set scatter plot properties (dots red)\n",
    "    line_kws = {'color': 'blue'}    # Set regression line properties (line blue)\n",
    "    \n",
    "    \n",
    "    # Check if standard deviations are not too close to zero\n",
    "    if new_qb_df['touchdown'].std() > 1e-10 and new_qb_df[y].std() > 1e-10:\n",
    "        sns.regplot(data=new_qb_df, x='touchdown', y=y, scatter_kws=scatter_kws, line_kws=line_kws)\n",
    "        plt.title(f\"Correlation between Touchdowns and {y}\")\n",
    "        plt.xlabel('Touchdowns')\n",
    "        plt.ylabel(y)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Insufficient variability in data for p-value calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2402bb-cd08-4938-9696-86ad6f8c04f9",
   "metadata": {},
   "source": [
    "### Machine learning\n",
    "\n",
    "***\n",
    "\n",
    " - Simple linear model\n",
    " - Train with the previous year, test on next year\n",
    " - Save out-of-sample test results\n",
    " - Visualize and run basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49388b2-94c5-463a-9a94-68969c5431b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's load in machine learning and stats packages\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6f6e3-0404-4ea2-8afe-2e240a782c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Peek at data to be trained\n",
    "new_qb_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f6e7d-8f4d-4917-a590-895bbeac45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will \"train\" our data on one season and \"test\"\n",
    "# on the next season. Train Test Splitting\n",
    "# codebasics tutorial https://www.youtube.com/watch?v=fwY9Qv96DJY\n",
    "\n",
    "# Let's use the previous season performance as\n",
    "# our features for our model \n",
    "features = ['pass_prev', 'complete_pass_prev', \n",
    "            'interception_prev', 'sack_prev',\n",
    "            'yards_gained_prev', 'touchdown_prev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ceaeb0-8325-4517-bbba-f460f13fefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are predicting \"current\" season touchdowns \n",
    "target = 'touchdown' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d23d11-805b-456e-a977-ac192f855a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a subset of data that has \n",
    "# no null values\n",
    "model_data = (new_qb_df\n",
    "              .dropna(subset=features+[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781a30b-3857-4f89-a808-3ff747ab3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on 2021 data  \n",
    "train_data = (model_data\n",
    "              .loc[model_data['season']==2021])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4c31b-4a06-445e-abd0-82d8d4114688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on 2022 data (data the model hasn't seen)\n",
    "test_data = (model_data\n",
    "             .loc[model_data['season']==2022])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a20d5-2356-49de-9328-1213a3d82c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the linear regression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95abf082-693f-4ee6-a55b-4bf9bd11feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit, or \"train\", the model on the training data\n",
    "model.fit(train_data.loc[:, features], \n",
    "          train_data[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647643cf-6906-4a34-add9-940a7e2dca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test data \n",
    "preds = model.predict(test_data.loc[:, features])\n",
    "\n",
    "#Take a peek\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed5ee0-8baf-4bf1-a02b-c5d5afbfffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set an index so predictions match the correct rows\n",
    "preds = pd.Series(preds, index=test_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca4380-5913-4763-b248-4ba007185767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the predictions back to your test dataset\n",
    "test_data['preds'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45fa555-722a-415e-bd07-975818373819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run some basic statistics to examine the\n",
    "# quality of the prediction\n",
    "\n",
    "rmse = mean_squared_error(test_data['touchdown'], test_data['preds'])**0.5\n",
    "r2 = pearsonr(test_data['touchdown'], test_data['preds'])[0]**2\n",
    "print(f\"rmse: {rmse}\\nr2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd5a71-7c89-448b-9629-28a3f50c6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the outputs \n",
    "plt.figure(figsize=(8, 6))  # Set figsize\n",
    "\n",
    "scatter_kws = {'color': 'red'}  # Set scatter plot properties (dots red)\n",
    "line_kws = {'color': 'blue'}    # Set regression line properties (line blue)\n",
    "\n",
    "sns.regplot(data=test_data, x='touchdown', y='preds', scatter_kws=scatter_kws, line_kws=line_kws)\n",
    "plt.title('Correlation between Touchdowns and Predictions')\n",
    "plt.xlabel('Touchdowns')\n",
    "plt.ylabel('Predictions')\n",
    "plt.legend(['Regression Line'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417497c3-11ab-45da-a9ce-bf81062624f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.loc[:, ['season', 'passer_id', 'passer', 'touchdown', 'preds']].sort_values('touchdown', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e659ef-c3ab-42b3-a3d7-aed4f598a84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c01c50-a02d-4510-b3c3-27ba5498787e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aed855-1cb0-4dcd-8162-fc15cb3e2f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e7bab40-4aa2-4d90-b60b-67882727162a",
   "metadata": {},
   "source": [
    "### Refrences\n",
    "1. https://www.scribbr.com/statistics/simple-linear-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba428b0-780a-4d38-82f8-1cf668a69180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98281a54-3b5c-4faf-a6a3-b9715f94f77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e8b10-3f6e-4732-ba45-d1a3f4fa3740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3f690-7be6-4523-907c-f84a3f172df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc15eeb8-b63e-42e4-9212-acb476fb72a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
